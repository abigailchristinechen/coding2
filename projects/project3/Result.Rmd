---
title: "Full Web Scraping for Awwwards"
author: "Abigail Chen"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
---

# Why Awwwards?
The goal of this project is to scrape the site [**Awwwards**](https://www.awwwards.com).  It is a website primarily focused on design, creativity and innovation for various platforms like web sites, mobile sites, AR/VR and other things on the internet.  This is really interesting because we can see the design trends, and digital visuals that are considered "best" depending on different criteria like "Design", "Usability", "Content", "Creativity", "Mobile", "Developer", then there will be a "Total Rating" which will rank the site for the awards.  Awwwards, is used as a design benchmark in the design community.  Designers and non-designers use it to find inspirations for their own projects. You can also see the technologies people use as the it evolves to build their digital products like Figma, API's, PHP, Shopify, various Javascripts, Laravel, Webpack, CSS, and others.  You can also see the top Countries ranked, and you can check out which country/ agency is good in designing what kinds of interfaces.  There aren't a lot of datasets for design, and I intend to start with this to build my own datasets, with the goal of making digital design "smarter" and more "intuitive" by being powered by data.  


# Scope & Continuinty 
For this project, I will be scraping the ALL of the yearly winners, monthly winners, daily winners, and nominees from Awwwards.  The data will be dating from 2011 up to December 2021. The graphs and representations here will give a very basic overview of the dataset, but I intend to continue and perform extended analysis to analyze the trends and the evolution of digital design. 


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(plotly)
library(prettydoc)
NomineesData = read.csv("NomineesData.csv")
CountryList = c("Man","MORE","Suriname")
NomineesData = NomineesData[!NomineesData$Country %in% CountryList ,]
NomineesData$Country = as.factor(NomineesData$Country)

MonthData = read.csv("MonthData.csv")
MonthData$Country = as.factor(MonthData$Country)

YearData = read.csv("YearData.csv")
YearData$Country = as.factor(YearData$Country)

DayData = read.csv("DayData.csv")
DayData$Country = as.factor(DayData$Country)
DayData = DayData[order(DayData$Country),]
DayData = DayData[5:nrow(DayData),]
```


# Scraping Methodology 
  1.    All the articles link for "Daily", "Monthly" and "Yearly" winners, along with the "Nominated" articles were scrapped and their web links were saved in a separate ".txt" file. 
  2.    The ".txt" files were read to get the list of web links saved earlier, these links were used to scrape "Title", "Description", "Country", "Author","Rating", "Categories" and "Colors" for "Daily", "Monthly" and "Yearly" winners. The scrapped data was saved as "csv" files.
  3.    For "Nominees" the "Rating" was not available, colors and authors couldn't be scrapped and rest was done as mentioned above. 


```{r }
head(NomineesData)
```


```{r }
head(DayData)
```


```{r }
head(MonthData)
```


```{r }
head(YearData)
```


# Scraping Results
##Total Winners  :   4,710 rows.
Daily Winners    :   4,526 rows. Monthly Winners  :   147 rows. Yearly Winners   :   37 rows.

##Nominees       :   10,140 rows.

##Total Scraped  :   14,850 rows.


